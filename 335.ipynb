{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "335.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekkbdMLLPEZ-"
      },
      "source": [
        "# Module 3: Machine Learning\n",
        "\n",
        "## Sprint 3: Introduction to Natural Language Processing and Computer Vision\n",
        "\n",
        "## Kaggle competition - don't overfit!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tct1VA3EPEaQ"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQwHL_9vPEaQ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs1xctJKPEaS"
      },
      "source": [
        "Participating in Kaggle competitions is an efficient way to learn some aspects of Machine Learning. You can read solutions made public by the others, participate in the discussions to talk about solution ideas and test them by submitting them for evaluation.\n",
        "\n",
        "The metric used for evaluation can vary from competition to competition, but the idea remains the same - build a model that is as accurate as possible on the testing set. In industry, there are other factors to consider when building machine learning models - inference time, solution complexity, maintainability and so on. However, even though you only learn a subset of the required skills while participating in Kaggle competitions, it is quite a fun way to learn by doing it, so let's participate in one of the competitions again!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jvAExg-PEaT"
      },
      "source": [
        "## The competition\n",
        "\n",
        "Even though we spent quite some time on natural language processing and computer vision during the sprint, the most accurate models on these types of data usually involves deep learning, that you will learn about in the upcoming course! In this project, the main goal will be to understand the concept of overfitting as deeply as possible, which essentially means fitting the training data very well at the expense of a model that generalizes and works well on other samples.\n",
        "\n",
        "To learn about the concept of overfitting, we will participate in the following Kaggle competition:\n",
        "\n",
        "- https://www.kaggle.com/c/dont-overfit-ii\n",
        "\n",
        "IMPORTANT: download the data from here - https://www.kaggle.com/sahiltinky/org-dataset-dont-overfitii, as the evaluation is done on an older dataset version than the one available at the competition data section.\n",
        "\n",
        "For help, you can look at some of the notebooks by other competitors. However, try to write code by yourself, as even though you will always be able to consult external resources while working as a professional, the main thing right now is to learn by first trying it yourself.\n",
        "\n",
        "Some notebooks that are worth exploring:\n",
        "\n",
        "- https://www.kaggle.com/artgor/how-to-not-overfit\n",
        "- https://www.kaggle.com/rafjaa/dealing-with-very-small-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os9J53mWPEaX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXJMPhKCPEaY"
      },
      "source": [
        "## Concepts to explore\n",
        "\n",
        "- https://towardsdatascience.com/how-to-improve-your-kaggle-competition-leaderboard-ranking-bcd16643eddf\n",
        "- https://opendatascience.com/10-tips-to-get-started-with-kaggle/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjD0AD-GPEaY"
      },
      "source": [
        "## Requirements\n",
        "\n",
        "- Data exploration\n",
        "- Feature engineering\n",
        "- At least several different models built and compared to each other on the validation set and on the public and private leaderboards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a2wdW8NPEaZ"
      },
      "source": [
        "## Evaluation criteria\n",
        "\n",
        "- Private leaderboard score (target is better than 0.8)\n",
        "- How simple is the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHuUUk3YPEaa"
      },
      "source": [
        "\n",
        "## Sample correction questions\n",
        "\n",
        "During a correction, you may get asked questions that test your understanding of covered topics.\n",
        "\n",
        "- Is it possible to use standard machine learning algorithms, such as logistic regression and random forests, when working with text? If yes, what has to be done and how?\n",
        "- You train a machine learning model and get a low validation accuracy. What other metrics you could check to better understand the problem? What are some of the ways to improve the validation accuracy?\n",
        "- How does looking at the validation accuracy, confusion matrix and important features complement each other when evaluating the model's performance?\n",
        "- How to make sure that the model that was deployed to production performs well?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jac5nxfQPGyi"
      },
      "source": [
        "## Notes Regarding Model Evaluation Metric\n",
        "\n",
        "ROC curves make it easy to identify the best threshold for making a decision. \n",
        "AUC allows for better comparing between different models.\n",
        "\n",
        "Although ROC graps are drawn using True Positive Rates(sensitivity) and False Positive Rates(1-specificity) to summarize confusion matrices, there are other metrics that attempt to do the same thing. \n",
        "\n",
        "For eample, people often replace the False Positive Rate with Precision. Precision is the true positives divided by the sum of true positives and false positives. Precision is the proportion of positive reslts that were corretly classified. If there is a class imbalance like there is in the data set (more zeroes than ones), then precision might ve more useful than the False Positive Rate."
      ]
    }
  ]
}